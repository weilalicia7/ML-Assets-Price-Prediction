"""
Phase 5 Production-Grade Improvements

Implements 6 production-grade enhancements from 'final improvement for phase5 on C model.pdf':
1. Real-Time Performance Monitoring & Alerting (+1-2% risk-adjusted)
2. Adaptive Parameter Optimization (+2-3% performance)
3. Cross-Validation for Regime Detection (+0.5-1% robustness)
4. Advanced Correlation Regime Detection (+1-2% crisis protection)
5. Portfolio-Level Risk Budgeting (+1-2% risk management)
6. ML Enhancement with XGBoost (+1-2% long-term)

Total Expected Improvement: +6-11% additional performance
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from collections import deque
from datetime import datetime
import logging

# Configure logging
logger = logging.getLogger(__name__)


# =============================================================================
# 1. REAL-TIME PERFORMANCE MONITORING & ALERTING
# =============================================================================

@dataclass
class PerformanceAlert:
    """Alert generated by performance monitor."""
    alert_type: str
    message: str
    severity: str  # 'info', 'warning', 'critical'
    timestamp: datetime
    metrics: Dict[str, float]


class Phase5PerformanceMonitor:
    """
    Real-time monitoring of Phase 5 improvements.

    Tracks effectiveness of each Phase 5 component and alerts
    when performance degrades below acceptable thresholds.
    """

    def __init__(
        self,
        alert_threshold: float = 0.4,
        lookback_window: int = 20,
        alert_callback: Optional[Callable[[PerformanceAlert], None]] = None
    ):
        """
        Initialize performance monitor.

        Args:
            alert_threshold: Minimum success rate before alerting
            lookback_window: Number of observations for rolling metrics
            alert_callback: Optional callback function for alerts
        """
        self.alert_threshold = alert_threshold
        self.lookback_window = lookback_window
        self.alert_callback = alert_callback

        self.performance_metrics = {
            'macro_adjustment_effectiveness': deque(maxlen=100),
            'regime_detection_accuracy': deque(maxlen=100),
            'kelly_sizing_performance': deque(maxlen=100),
            'diversification_benefit': deque(maxlen=100),
            'timeframe_agreement_accuracy': deque(maxlen=100),
            'overall_signal_accuracy': deque(maxlen=100)
        }

        self.alerts_history: List[PerformanceAlert] = []
        self.last_alert_time: Dict[str, datetime] = {}
        self.alert_cooldown_minutes = 60  # Avoid alert spam

    def track_macro_adjustment_effectiveness(
        self,
        original_score: float,
        adjusted_score: float,
        actual_return: float
    ) -> None:
        """
        Track if macro adjustments improve predictions.

        Args:
            original_score: Score before macro adjustment
            adjusted_score: Score after macro adjustment
            actual_return: Actual realized return
        """
        # Effectiveness = 1 if adjustment moved score in correct direction
        original_alignment = original_score * actual_return
        adjusted_alignment = adjusted_score * actual_return

        effectiveness = 1.0 if adjusted_alignment > original_alignment else -1.0
        self.performance_metrics['macro_adjustment_effectiveness'].append(effectiveness)

        # Check for alert
        if len(self.performance_metrics['macro_adjustment_effectiveness']) >= self.lookback_window:
            recent = list(self.performance_metrics['macro_adjustment_effectiveness'])[-self.lookback_window:]
            success_rate = np.mean([1 if e > 0 else 0 for e in recent])

            if success_rate < self.alert_threshold:
                self._trigger_alert(
                    "MACRO_ADJUSTMENT_INEFFECTIVE",
                    f"Macro adjustment success rate: {success_rate:.1%} (threshold: {self.alert_threshold:.1%})",
                    "warning",
                    {'success_rate': success_rate, 'threshold': self.alert_threshold}
                )

    def track_regime_accuracy(
        self,
        predicted_regime: str,
        actual_volatility: float,
        historical_volatility: float
    ) -> None:
        """
        Validate regime detection accuracy.

        Args:
            predicted_regime: Predicted volatility regime
            actual_volatility: Realized volatility
            historical_volatility: Historical average volatility
        """
        actual_regime = self._classify_actual_regime(actual_volatility, historical_volatility)
        accuracy = 1.0 if predicted_regime == actual_regime else 0.0
        self.performance_metrics['regime_detection_accuracy'].append(accuracy)

        # Check for alert
        if len(self.performance_metrics['regime_detection_accuracy']) >= self.lookback_window:
            recent = list(self.performance_metrics['regime_detection_accuracy'])[-self.lookback_window:]
            accuracy_rate = np.mean(recent)

            if accuracy_rate < self.alert_threshold:
                self._trigger_alert(
                    "REGIME_DETECTION_POOR",
                    f"Regime detection accuracy: {accuracy_rate:.1%}",
                    "warning",
                    {'accuracy_rate': accuracy_rate}
                )

    def track_kelly_sizing_performance(
        self,
        position_size: float,
        actual_return: float,
        was_profitable: bool
    ) -> None:
        """
        Track Kelly position sizing performance.

        Args:
            position_size: Position size used
            actual_return: Actual return achieved
            was_profitable: Whether trade was profitable
        """
        # Score based on position size alignment with profitability
        if was_profitable:
            score = position_size * actual_return  # Reward larger positions on winners
        else:
            score = -position_size * abs(actual_return)  # Penalize larger positions on losers

        self.performance_metrics['kelly_sizing_performance'].append(score)

    def track_diversification_benefit(
        self,
        portfolio_return: float,
        benchmark_return: float,
        portfolio_volatility: float,
        benchmark_volatility: float
    ) -> None:
        """
        Track if diversification penalty improves risk-adjusted returns.

        Args:
            portfolio_return: Portfolio return
            benchmark_return: Benchmark return
            portfolio_volatility: Portfolio volatility
            benchmark_volatility: Benchmark volatility
        """
        # Risk-adjusted benefit
        portfolio_sharpe = portfolio_return / portfolio_volatility if portfolio_volatility > 0 else 0
        benchmark_sharpe = benchmark_return / benchmark_volatility if benchmark_volatility > 0 else 0

        benefit = portfolio_sharpe - benchmark_sharpe
        self.performance_metrics['diversification_benefit'].append(benefit)

    def track_signal_accuracy(
        self,
        signal_direction: float,
        actual_return: float
    ) -> None:
        """
        Track overall signal accuracy.

        Args:
            signal_direction: Predicted direction (-1 to 1)
            actual_return: Actual return
        """
        correct = 1.0 if (signal_direction * actual_return) > 0 else 0.0
        self.performance_metrics['overall_signal_accuracy'].append(correct)

    def _classify_actual_regime(
        self,
        actual_volatility: float,
        historical_volatility: float
    ) -> str:
        """Classify actual regime based on realized volatility."""
        vol_ratio = actual_volatility / historical_volatility if historical_volatility > 0 else 1.0

        if vol_ratio < 0.7:
            return 'low'
        elif vol_ratio < 1.3:
            return 'normal'
        elif vol_ratio < 2.0:
            return 'high'
        else:
            return 'crisis'

    def _trigger_alert(
        self,
        alert_type: str,
        message: str,
        severity: str,
        metrics: Dict[str, float]
    ) -> None:
        """Trigger a performance alert."""
        now = datetime.now()

        # Check cooldown
        if alert_type in self.last_alert_time:
            minutes_since = (now - self.last_alert_time[alert_type]).total_seconds() / 60
            if minutes_since < self.alert_cooldown_minutes:
                return

        alert = PerformanceAlert(
            alert_type=alert_type,
            message=message,
            severity=severity,
            timestamp=now,
            metrics=metrics
        )

        self.alerts_history.append(alert)
        self.last_alert_time[alert_type] = now

        logger.warning(f"Phase5 Alert [{severity.upper()}]: {alert_type} - {message}")

        if self.alert_callback:
            self.alert_callback(alert)

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get summary of all performance metrics."""
        summary = {}

        for metric_name, values in self.performance_metrics.items():
            if len(values) > 0:
                values_list = list(values)
                summary[metric_name] = {
                    'mean': np.mean(values_list),
                    'std': np.std(values_list),
                    'recent_mean': np.mean(values_list[-self.lookback_window:]) if len(values_list) >= self.lookback_window else np.mean(values_list),
                    'count': len(values_list)
                }
            else:
                summary[metric_name] = {'mean': 0, 'std': 0, 'recent_mean': 0, 'count': 0}

        summary['total_alerts'] = len(self.alerts_history)
        summary['recent_alerts'] = [a for a in self.alerts_history if (datetime.now() - a.timestamp).days < 1]

        return summary


# =============================================================================
# 2. ADAPTIVE PARAMETER OPTIMIZATION
# =============================================================================

class AdaptiveParameterOptimizer:
    """
    Continuously optimize Phase 5 parameters based on recent performance.

    Adjusts boost factors, Kelly fraction, and other parameters
    dynamically based on what's actually working.
    """

    def __init__(
        self,
        optimization_lookback: int = 63,
        min_samples: int = 20,
        learning_rate: float = 0.1
    ):
        """
        Initialize adaptive optimizer.

        Args:
            optimization_lookback: Days of history for optimization
            min_samples: Minimum samples before optimization
            learning_rate: How fast to adjust parameters (0-1)
        """
        self.optimization_lookback = optimization_lookback
        self.min_samples = min_samples
        self.learning_rate = learning_rate

        # Performance tracking by asset class and regime
        self.performance_by_asset_class: Dict[str, Dict[str, List[float]]] = {}
        self.performance_by_regime: Dict[str, List[float]] = {
            'risk_on': [],
            'risk_off': []
        }

        # Current optimized parameters
        self.optimized_boosts: Dict[str, Dict[str, float]] = {}
        self.optimized_kelly_fraction = 0.25

        # Default boost factors
        self.default_risk_on_boosts = {
            'equity': 1.3, 'crypto': 1.2, 'etf': 1.1,
            'international': 1.1, 'bond': 0.8, 'commodity': 0.9, 'forex': 0.95
        }
        self.default_risk_off_boosts = {
            'bond': 1.3, 'commodity': 1.2, 'equity': 0.7,
            'crypto': 0.5, 'etf': 0.9, 'forex': 1.1, 'international': 0.8
        }

    def record_performance(
        self,
        asset_class: str,
        regime: str,
        return_value: float
    ) -> None:
        """
        Record performance for later optimization.

        Args:
            asset_class: Asset class
            regime: 'risk_on' or 'risk_off'
            return_value: Realized return
        """
        if asset_class not in self.performance_by_asset_class:
            self.performance_by_asset_class[asset_class] = {
                'risk_on': [],
                'risk_off': []
            }

        self.performance_by_asset_class[asset_class][regime].append(return_value)
        self.performance_by_regime[regime].append(return_value)

        # Trim to lookback window
        for ac in self.performance_by_asset_class:
            for r in ['risk_on', 'risk_off']:
                if len(self.performance_by_asset_class[ac][r]) > self.optimization_lookback:
                    self.performance_by_asset_class[ac][r] = \
                        self.performance_by_asset_class[ac][r][-self.optimization_lookback:]

        for r in ['risk_on', 'risk_off']:
            if len(self.performance_by_regime[r]) > self.optimization_lookback:
                self.performance_by_regime[r] = self.performance_by_regime[r][-self.optimization_lookback:]

    def optimize_macro_boost_factors(self) -> Dict[str, Dict[str, float]]:
        """
        Dynamically adjust risk-on/risk-off boost factors.

        Returns:
            Optimized boost factors by asset class
        """
        optimized_boosts = {}

        for asset_class, perf_data in self.performance_by_asset_class.items():
            risk_on_returns = perf_data.get('risk_on', [])
            risk_off_returns = perf_data.get('risk_off', [])

            # Calculate average performance in each regime
            risk_on_perf = np.mean(risk_on_returns) if len(risk_on_returns) >= self.min_samples else 0.0
            risk_off_perf = np.mean(risk_off_returns) if len(risk_off_returns) >= self.min_samples else 0.0

            # Adaptive boosting based on actual performance
            # Scale performance to boost factor (centered at 1.0)
            risk_on_boost = 1.0 + (risk_on_perf * 20.0)  # Scale factor
            risk_off_boost = 1.0 + (risk_off_perf * 20.0)

            # Blend with defaults using learning rate
            default_risk_on = self.default_risk_on_boosts.get(asset_class, 1.0)
            default_risk_off = self.default_risk_off_boosts.get(asset_class, 1.0)

            blended_risk_on = (1 - self.learning_rate) * default_risk_on + self.learning_rate * risk_on_boost
            blended_risk_off = (1 - self.learning_rate) * default_risk_off + self.learning_rate * risk_off_boost

            # Bound to reasonable range
            optimized_boosts[asset_class] = {
                'risk_on': max(0.5, min(2.0, blended_risk_on)),
                'risk_off': max(0.5, min(2.0, blended_risk_off))
            }

        self.optimized_boosts = optimized_boosts
        return optimized_boosts

    def optimize_kelly_fraction(
        self,
        recent_trade_results: List[float]
    ) -> float:
        """
        Dynamically adjust Kelly fraction based on recent risk-adjusted returns.

        Args:
            recent_trade_results: Recent trade returns

        Returns:
            Optimized Kelly fraction
        """
        if len(recent_trade_results) < self.min_samples:
            return self.optimized_kelly_fraction

        sharpe_ratios = []

        for period in [21, 63, 126]:  # Different lookbacks
            if len(recent_trade_results) >= period:
                period_returns = recent_trade_results[-period:]
                if len(period_returns) > 10:
                    std_return = np.std(period_returns)
                    if std_return > 0:
                        sharpe = np.mean(period_returns) / std_return
                        sharpe_ratios.append(sharpe)

        if not sharpe_ratios:
            return self.optimized_kelly_fraction

        avg_sharpe = np.mean(sharpe_ratios)

        # Adjust Kelly fraction based on risk-adjusted performance
        if avg_sharpe > 1.0:
            optimal_fraction = 0.30  # Slightly more aggressive
        elif avg_sharpe > 0.5:
            optimal_fraction = 0.25  # Standard quarter-Kelly
        elif avg_sharpe > 0:
            optimal_fraction = 0.20  # More conservative
        else:
            optimal_fraction = 0.15  # Very conservative

        # Blend with current
        self.optimized_kelly_fraction = (
            (1 - self.learning_rate) * self.optimized_kelly_fraction +
            self.learning_rate * optimal_fraction
        )

        return max(0.1, min(0.4, self.optimized_kelly_fraction))

    def get_optimized_parameters(self) -> Dict[str, Any]:
        """Get all optimized parameters."""
        return {
            'boost_factors': self.optimized_boosts,
            'kelly_fraction': self.optimized_kelly_fraction,
            'samples_collected': {
                ac: {r: len(v) for r, v in data.items()}
                for ac, data in self.performance_by_asset_class.items()
            }
        }


# =============================================================================
# 3. CROSS-VALIDATION FOR REGIME DETECTION
# =============================================================================

class RegimeDetectionValidator:
    """
    Validate and improve regime detection accuracy through cross-validation.
    """

    def __init__(self):
        """Initialize regime detection validator."""
        self.validated_params: Dict[str, Any] = {}
        self.validation_results: List[Dict[str, Any]] = []

        # Default thresholds
        self.default_thresholds = {
            'low': 0.7,
            'normal': 1.3,
            'high': 2.0
        }

    def cross_validate_regime_params(
        self,
        historical_data: pd.DataFrame,
        returns_column: str = 'returns'
    ) -> Dict[str, Any]:
        """
        Find optimal parameters for regime detection across different market periods.

        Args:
            historical_data: DataFrame with historical returns
            returns_column: Column name for returns

        Returns:
            Best parameters for each validation period
        """
        if returns_column not in historical_data.columns:
            return {'error': 'Returns column not found'}

        returns = historical_data[returns_column].dropna()

        if len(returns) < 126:  # Need at least 6 months
            return {'error': 'Insufficient data'}

        # Split into validation windows
        window_size = len(returns) // 4
        validation_windows = [
            ('period_1', returns.iloc[:window_size]),
            ('period_2', returns.iloc[window_size:window_size*2]),
            ('period_3', returns.iloc[window_size*2:window_size*3]),
            ('period_4', returns.iloc[window_size*3:])
        ]

        # Threshold candidates to test
        threshold_candidates = [
            {'low': 0.6, 'normal': 1.2, 'high': 2.0},   # Conservative
            {'low': 0.7, 'normal': 1.3, 'high': 1.8},   # Moderate
            {'low': 0.8, 'normal': 1.4, 'high': 1.6},   # Aggressive
        ]

        best_params = {}

        for period_name, period_data in validation_windows:
            best_threshold = None
            best_accuracy = 0

            for thresholds in threshold_candidates:
                accuracy = self._validate_regime_accuracy(period_data, thresholds)
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_threshold = thresholds

            optimal_lookback = self._find_optimal_lookback(period_data)

            best_params[period_name] = {
                'thresholds': best_threshold or self.default_thresholds,
                'accuracy': best_accuracy,
                'optimal_lookback': optimal_lookback
            }

        # Synthesize adaptive parameters
        self.validated_params = self._synthesize_adaptive_params(best_params)
        return self.validated_params

    def _validate_regime_accuracy(
        self,
        returns: pd.Series,
        thresholds: Dict[str, float]
    ) -> float:
        """Validate regime accuracy with given thresholds."""
        if len(returns) < 40:
            return 0.0

        historical_vol = returns.std() * np.sqrt(252)
        correct_predictions = 0
        total_predictions = 0

        for i in range(20, len(returns) - 5):
            # Predict regime
            recent_vol = returns.iloc[i-20:i].std() * np.sqrt(252)
            vol_ratio = recent_vol / historical_vol if historical_vol > 0 else 1.0

            if vol_ratio < thresholds['low']:
                predicted = 'low'
            elif vol_ratio < thresholds['normal']:
                predicted = 'normal'
            elif vol_ratio < thresholds['high']:
                predicted = 'high'
            else:
                predicted = 'crisis'

            # Actual regime (forward looking)
            future_vol = returns.iloc[i:i+5].std() * np.sqrt(252)
            future_ratio = future_vol / historical_vol if historical_vol > 0 else 1.0

            if future_ratio < thresholds['low']:
                actual = 'low'
            elif future_ratio < thresholds['normal']:
                actual = 'normal'
            elif future_ratio < thresholds['high']:
                actual = 'high'
            else:
                actual = 'crisis'

            if predicted == actual:
                correct_predictions += 1
            total_predictions += 1

        return correct_predictions / total_predictions if total_predictions > 0 else 0.0

    def _find_optimal_lookback(self, returns: pd.Series) -> int:
        """Find optimal lookback period for volatility calculation."""
        lookbacks = [10, 15, 20, 30, 40]
        best_lookback = 20
        best_stability = 0

        for lb in lookbacks:
            if len(returns) < lb + 20:
                continue

            # Calculate rolling volatility stability
            rolling_vol = returns.rolling(lb).std()
            vol_changes = rolling_vol.diff().abs().dropna()
            stability = 1 / (vol_changes.mean() + 0.001)

            if stability > best_stability:
                best_stability = stability
                best_lookback = lb

        return best_lookback

    def _synthesize_adaptive_params(
        self,
        period_params: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Synthesize best parameters across all periods."""
        # Average the thresholds weighted by accuracy
        total_weight = sum(p['accuracy'] for p in period_params.values())

        if total_weight == 0:
            return {
                'thresholds': self.default_thresholds,
                'lookback': 20,
                'validation_accuracy': 0
            }

        synthesized_thresholds = {'low': 0, 'normal': 0, 'high': 0}
        avg_lookback = 0

        for period_name, params in period_params.items():
            weight = params['accuracy'] / total_weight
            for key in ['low', 'normal', 'high']:
                synthesized_thresholds[key] += params['thresholds'][key] * weight
            avg_lookback += params['optimal_lookback'] * weight

        return {
            'thresholds': synthesized_thresholds,
            'lookback': int(avg_lookback),
            'validation_accuracy': total_weight / len(period_params),
            'period_results': period_params
        }


# =============================================================================
# 4. ADVANCED CORRELATION REGIME DETECTION
# =============================================================================

class CorrelationRegimeDetector:
    """
    Detect when correlation structures break down (crisis periods).

    Uses correlation matrix entropy to identify regime changes.
    """

    def __init__(
        self,
        entropy_threshold_high: float = 0.15,
        entropy_threshold_low: float = -0.10,
        lookback_days: int = 63
    ):
        """
        Initialize correlation regime detector.

        Args:
            entropy_threshold_high: Threshold for correlation breakdown
            entropy_threshold_low: Threshold for correlation normalization
            lookback_days: Days for entropy trend analysis
        """
        self.entropy_threshold_high = entropy_threshold_high
        self.entropy_threshold_low = entropy_threshold_low
        self.lookback_days = lookback_days

        self.entropy_history: deque = deque(maxlen=252)
        self.regime_history: List[str] = []

    def monitor_correlation_stability(
        self,
        correlation_matrix: np.ndarray
    ) -> str:
        """
        Track stability of correlation matrix to detect regime changes.

        Args:
            correlation_matrix: Current correlation matrix

        Returns:
            Correlation regime: 'stable', 'correlation_breakdown', or 'correlation_normalization'
        """
        current_entropy = self.calculate_correlation_entropy(correlation_matrix)
        self.entropy_history.append(current_entropy)

        if len(self.entropy_history) < 5:
            return 'stable'

        entropy_trend = self._analyze_entropy_trend()

        if entropy_trend > self.entropy_threshold_high:
            regime = 'correlation_breakdown'
        elif entropy_trend < self.entropy_threshold_low:
            regime = 'correlation_normalization'
        else:
            regime = 'stable'

        self.regime_history.append(regime)
        return regime

    def calculate_correlation_entropy(self, corr_matrix: np.ndarray) -> float:
        """
        Calculate entropy of correlation matrix as stability measure.

        Higher entropy = more unstable correlations
        Lower entropy = more stable, predictable correlations

        Args:
            corr_matrix: Correlation matrix

        Returns:
            Entropy value
        """
        try:
            # Get eigenvalues
            eigenvalues = np.linalg.eigvals(corr_matrix)
            eigenvalues = np.real(eigenvalues)  # Take real part
            eigenvalues = np.abs(eigenvalues)   # Take absolute value

            # Normalize
            eigenvalues = eigenvalues / (np.sum(eigenvalues) + 1e-10)

            # Calculate entropy
            entropy = -np.sum(eigenvalues * np.log(eigenvalues + 1e-10))

            return float(entropy)
        except Exception:
            return 0.0

    def _analyze_entropy_trend(self) -> float:
        """Analyze trend in entropy over lookback period."""
        if len(self.entropy_history) < 5:
            return 0.0

        recent = list(self.entropy_history)[-min(self.lookback_days, len(self.entropy_history)):]

        if len(recent) < 5:
            return 0.0

        # Calculate trend as slope of entropy
        x = np.arange(len(recent))
        slope = np.polyfit(x, recent, 1)[0]

        return slope

    def get_correlation_regime_multiplier(self, regime: str) -> float:
        """
        Get position sizing multiplier based on correlation regime.

        Args:
            regime: Current correlation regime

        Returns:
            Position size multiplier
        """
        multipliers = {
            'stable': 1.0,
            'correlation_normalization': 1.1,   # Increasing stability
            'correlation_breakdown': 0.3        # Crisis - very conservative
        }
        return multipliers.get(regime, 1.0)

    def get_regime_statistics(self) -> Dict[str, Any]:
        """Get statistics about correlation regimes."""
        if not self.regime_history:
            return {'regime_counts': {}, 'current_entropy': 0}

        regime_counts = {}
        for regime in self.regime_history:
            regime_counts[regime] = regime_counts.get(regime, 0) + 1

        return {
            'regime_counts': regime_counts,
            'current_entropy': self.entropy_history[-1] if self.entropy_history else 0,
            'entropy_trend': self._analyze_entropy_trend(),
            'history_length': len(self.regime_history)
        }


# =============================================================================
# 5. PORTFOLIO-LEVEL RISK BUDGETING
# =============================================================================

class PortfolioRiskBudget:
    """
    Manage risk budget across entire portfolio considering Phase 5 enhancements.
    """

    def __init__(
        self,
        total_risk_budget: float = 1.0,
        max_single_position_risk: float = 0.25,
        min_position_risk: float = 0.02
    ):
        """
        Initialize portfolio risk budget.

        Args:
            total_risk_budget: Total risk budget (1.0 = 100%)
            max_single_position_risk: Maximum risk for single position
            min_position_risk: Minimum risk allocation
        """
        self.total_risk_budget = total_risk_budget
        self.max_single_position_risk = max_single_position_risk
        self.min_position_risk = min_position_risk

        self.position_risk_scores: Dict[str, float] = {}
        self.risk_allocation_history: List[Dict[str, float]] = []

    def calculate_risk_budget(
        self,
        portfolio: Dict[str, float],
        macro_context: Dict[str, Any],
        correlation_matrix: Optional[np.ndarray] = None,
        ticker_volatilities: Optional[Dict[str, float]] = None
    ) -> Dict[str, float]:
        """
        Dynamic risk budgeting considering all Phase 5 factors.

        Args:
            portfolio: Current portfolio {ticker: weight}
            macro_context: Macro indicators
            correlation_matrix: Optional correlation matrix
            ticker_volatilities: Optional volatility by ticker

        Returns:
            Risk budget allocation by ticker
        """
        if not portfolio:
            return {}

        allocated_risk = 0.0
        position_risk_scores = {}

        for ticker, position in portfolio.items():
            # Base risk score
            base_risk = self._calculate_position_risk(
                ticker, position, ticker_volatilities
            )

            # Macro adjustment
            macro_multiplier = self._get_macro_risk_multiplier(macro_context)

            # Diversification benefit
            diversification_benefit = self._calculate_diversification_benefit(
                ticker, portfolio, correlation_matrix
            )

            # Combined risk score
            risk_score = base_risk * macro_multiplier * (1 - diversification_benefit)
            risk_score = max(self.min_position_risk, min(self.max_single_position_risk, risk_score))

            position_risk_scores[ticker] = risk_score
            allocated_risk += risk_score

        # Normalize to risk budget
        if allocated_risk > self.total_risk_budget:
            scaling_factor = self.total_risk_budget / allocated_risk
            position_risk_scores = {
                k: v * scaling_factor for k, v in position_risk_scores.items()
            }

        self.position_risk_scores = position_risk_scores
        self.risk_allocation_history.append(position_risk_scores.copy())

        return position_risk_scores

    def _calculate_position_risk(
        self,
        ticker: str,
        position: float,
        ticker_volatilities: Optional[Dict[str, float]] = None
    ) -> float:
        """Calculate base risk for a position."""
        # Default volatility if not provided
        volatility = 0.20  # 20% annualized default

        if ticker_volatilities and ticker in ticker_volatilities:
            volatility = ticker_volatilities[ticker]

        # Risk = position size * volatility
        return position * volatility

    def _get_macro_risk_multiplier(self, macro_context: Dict[str, Any]) -> float:
        """Adjust risk based on macro conditions."""
        vix = macro_context.get('vix', 20.0)

        if vix < 15:
            return 1.2   # Low fear - increase risk budget
        elif vix < 25:
            return 1.0   # Normal
        elif vix < 35:
            return 0.7   # Elevated fear - reduce risk
        else:
            return 0.4   # Crisis - minimal risk

    def _calculate_diversification_benefit(
        self,
        ticker: str,
        portfolio: Dict[str, float],
        correlation_matrix: Optional[np.ndarray] = None
    ) -> float:
        """Calculate diversification benefit for a position."""
        if correlation_matrix is None or len(portfolio) < 2:
            return 0.0

        # Simple diversification benefit based on number of holdings
        # More sophisticated implementation would use actual correlations
        n_holdings = len(portfolio)
        base_benefit = min(0.5, (n_holdings - 1) * 0.1)  # Up to 50% benefit

        return base_benefit

    def get_remaining_budget(self) -> float:
        """Get remaining risk budget."""
        used = sum(self.position_risk_scores.values())
        return max(0, self.total_risk_budget - used)

    def get_budget_summary(self) -> Dict[str, Any]:
        """Get risk budget summary."""
        return {
            'total_budget': self.total_risk_budget,
            'allocated': sum(self.position_risk_scores.values()),
            'remaining': self.get_remaining_budget(),
            'position_allocations': self.position_risk_scores,
            'history_length': len(self.risk_allocation_history)
        }


# =============================================================================
# 6. ML ENHANCEMENT (XGBoost integration)
# =============================================================================

class MLEnhancedPhase5:
    """
    Use ML to improve Phase 5 decision making.

    Combines traditional Phase 5 signals with ML predictions
    for enhanced accuracy.
    """

    def __init__(
        self,
        ml_weight: float = 0.3,
        confidence_threshold_high: float = 0.7,
        confidence_threshold_low: float = 0.3
    ):
        """
        Initialize ML-enhanced Phase 5.

        Args:
            ml_weight: Weight for ML predictions vs traditional
            confidence_threshold_high: Threshold for high ML confidence
            confidence_threshold_low: Threshold for low ML confidence
        """
        self.ml_weight = ml_weight
        self.confidence_threshold_high = confidence_threshold_high
        self.confidence_threshold_low = confidence_threshold_low

        self.model = None
        self.feature_names = [
            'macro_adjusted_score',
            'regime_kelly_position',
            'diversification_penalty',
            'timeframe_agreement',
            'correlation_regime',
            'vix_level',
            'market_trend'
        ]
        self.training_data: List[Dict[str, Any]] = []
        self.is_trained = False

    def prepare_features(self, current_signals: Dict[str, Any]) -> np.ndarray:
        """
        Prepare feature vector for ML model.

        Args:
            current_signals: Dictionary of current signal values

        Returns:
            Feature vector
        """
        features = []

        for feature_name in self.feature_names:
            value = current_signals.get(feature_name, 0.0)

            # Convert categorical to numeric
            if feature_name == 'correlation_regime':
                regime_map = {'stable': 0, 'correlation_normalization': 1, 'correlation_breakdown': -1}
                value = regime_map.get(value, 0)

            features.append(float(value))

        return np.array(features)

    def train_model(
        self,
        historical_data: List[Dict[str, Any]],
        target_column: str = 'actual_profitability'
    ) -> bool:
        """
        Train ML model to optimally combine Phase 5 signals.

        Args:
            historical_data: List of historical signal dictionaries
            target_column: Column indicating trade profitability

        Returns:
            True if training successful
        """
        try:
            import xgboost as xgb
        except ImportError:
            logger.warning("XGBoost not installed. ML enhancement disabled.")
            return False

        if len(historical_data) < 100:
            logger.warning("Insufficient data for ML training")
            return False

        # Prepare training data
        X = []
        y = []

        for record in historical_data:
            features = self.prepare_features(record)
            target = record.get(target_column, 0)

            X.append(features)
            y.append(1 if target > 0 else 0)

        X = np.array(X)
        y = np.array(y)

        # Train model
        self.model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        self.model.fit(X, y)
        self.is_trained = True

        logger.info(f"ML model trained on {len(X)} samples")
        return True

    def get_ml_enhanced_decision(
        self,
        current_signals: Dict[str, Any],
        base_decision: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Use ML model to enhance Phase 5 decisions.

        Args:
            current_signals: Current signal values
            base_decision: Base Phase 5 decision

        Returns:
            Enhanced decision with ML adjustments
        """
        if not self.is_trained or self.model is None:
            return base_decision

        try:
            # Get ML probability
            features = self.prepare_features(current_signals).reshape(1, -1)
            ml_probability = self.model.predict_proba(features)[0][1]

            base_position = base_decision.get('position', 0)
            base_confidence = base_decision.get('confidence', 0.5)

            # Combine with ML confidence
            if ml_probability > self.confidence_threshold_high:
                # High ML confidence - boost position
                enhanced_position = base_position * 1.2
                enhanced_confidence = min(1.0, base_confidence * 1.1)
            elif ml_probability < self.confidence_threshold_low:
                # Low ML confidence - reduce position
                enhanced_position = base_position * 0.7
                enhanced_confidence = base_confidence * 0.8
            else:
                # Neutral ML view - use base decision
                enhanced_position = base_position
                enhanced_confidence = base_confidence

            return {
                'position': enhanced_position,
                'confidence': enhanced_confidence,
                'ml_probability': ml_probability,
                'base_decision': base_decision,
                'ml_enhanced': True
            }

        except Exception as e:
            logger.error(f"ML enhancement failed: {e}")
            return base_decision

    def add_training_sample(
        self,
        signals: Dict[str, Any],
        was_profitable: bool
    ) -> None:
        """Add a training sample for future model updates."""
        sample = signals.copy()
        sample['actual_profitability'] = 1 if was_profitable else 0
        self.training_data.append(sample)

    def get_feature_importance(self) -> Dict[str, float]:
        """Get feature importance from trained model."""
        if not self.is_trained or self.model is None:
            return {}

        try:
            importances = self.model.feature_importances_
            return dict(zip(self.feature_names, importances))
        except Exception:
            return {}


# =============================================================================
# INTEGRATED PRODUCTION PHASE 5 SYSTEM
# =============================================================================

class ProductionPhase5System:
    """
    Fully integrated production-grade Phase 5 system.

    Combines all 6 production improvements into a single system.
    """

    def __init__(
        self,
        base_system: Any = None,
        enable_ml: bool = True,
        enable_monitoring: bool = True
    ):
        """
        Initialize production Phase 5 system.

        Args:
            base_system: Base Phase 5 system (MacroEnhancedPhase5System)
            enable_ml: Enable ML enhancement
            enable_monitoring: Enable performance monitoring
        """
        self.base_system = base_system

        # Initialize all production components
        self.performance_monitor = Phase5PerformanceMonitor() if enable_monitoring else None
        self.parameter_optimizer = AdaptiveParameterOptimizer()
        self.regime_validator = RegimeDetectionValidator()
        self.correlation_detector = CorrelationRegimeDetector()
        self.risk_budget = PortfolioRiskBudget()
        self.ml_enhancer = MLEnhancedPhase5() if enable_ml else None

        # Configuration
        self.enable_ml = enable_ml
        self.enable_monitoring = enable_monitoring

        # State
        self.total_decisions = 0
        self.profitable_decisions = 0

    def generate_production_signal(
        self,
        ticker: str,
        data: pd.DataFrame,
        raw_signals: Dict[str, float],
        macro_context: Dict[str, Any],
        portfolio: Dict[str, float] = None,
        correlation_matrix: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """
        Generate production-grade trading signal.

        Args:
            ticker: Stock ticker
            data: Price data
            raw_signals: Raw signal values
            macro_context: Macro indicators
            portfolio: Current portfolio
            correlation_matrix: Optional correlation matrix

        Returns:
            Production-grade trading signal
        """
        # Step 1: Get optimized parameters
        optimized_params = self.parameter_optimizer.get_optimized_parameters()

        # Step 2: Detect correlation regime
        correlation_regime = 'stable'
        if correlation_matrix is not None:
            correlation_regime = self.correlation_detector.monitor_correlation_stability(
                correlation_matrix
            )

        # Step 3: Calculate risk budget
        risk_allocation = {}
        if portfolio:
            risk_allocation = self.risk_budget.calculate_risk_budget(
                portfolio, macro_context, correlation_matrix
            )

        # Step 4: Build signal features
        signal_features = {
            'macro_adjusted_score': raw_signals.get('composite_score', 0.5),
            'regime_kelly_position': raw_signals.get('kelly_position', 0.05),
            'diversification_penalty': raw_signals.get('diversification_penalty', 0),
            'timeframe_agreement': raw_signals.get('mtf_agreement', 0.5),
            'correlation_regime': correlation_regime,
            'vix_level': macro_context.get('vix', 20),
            'market_trend': macro_context.get('spy_trend', 0)
        }

        # Step 5: Generate base decision
        base_decision = {
            'ticker': ticker,
            'direction': raw_signals.get('direction', 0),
            'confidence': raw_signals.get('confidence', 0.5),
            'position': raw_signals.get('position_size', 0.05),
            'correlation_regime': correlation_regime,
            'risk_budget': risk_allocation.get(ticker, 0.1)
        }

        # Apply correlation regime multiplier
        corr_multiplier = self.correlation_detector.get_correlation_regime_multiplier(
            correlation_regime
        )
        base_decision['position'] *= corr_multiplier

        # Step 6: Apply ML enhancement if enabled
        final_decision = base_decision
        if self.enable_ml and self.ml_enhancer:
            final_decision = self.ml_enhancer.get_ml_enhanced_decision(
                signal_features, base_decision
            )

        # Step 7: Add metadata
        final_decision['optimized_params'] = optimized_params
        final_decision['correlation_stats'] = self.correlation_detector.get_regime_statistics()
        final_decision['risk_budget_summary'] = self.risk_budget.get_budget_summary()
        final_decision['production_grade'] = True

        self.total_decisions += 1

        return final_decision

    def record_outcome(
        self,
        ticker: str,
        signal: Dict[str, Any],
        actual_return: float,
        asset_class: str = 'equity'
    ) -> None:
        """
        Record trade outcome for learning and monitoring.

        Args:
            ticker: Stock ticker
            signal: Original signal
            actual_return: Actual realized return
            asset_class: Asset class
        """
        was_profitable = actual_return > 0

        if was_profitable:
            self.profitable_decisions += 1

        # Update parameter optimizer
        regime = 'risk_off' if signal.get('correlation_regime') == 'correlation_breakdown' else 'risk_on'
        self.parameter_optimizer.record_performance(asset_class, regime, actual_return)

        # Update performance monitor
        if self.performance_monitor:
            self.performance_monitor.track_signal_accuracy(
                signal.get('direction', 0),
                actual_return
            )

            self.performance_monitor.track_kelly_sizing_performance(
                signal.get('position', 0),
                actual_return,
                was_profitable
            )

        # Add ML training sample
        if self.ml_enhancer:
            self.ml_enhancer.add_training_sample(signal, was_profitable)

    def optimize_parameters(self, trade_results: List[float]) -> Dict[str, Any]:
        """Trigger parameter optimization."""
        self.parameter_optimizer.optimize_macro_boost_factors()
        kelly_fraction = self.parameter_optimizer.optimize_kelly_fraction(trade_results)

        return {
            'boost_factors': self.parameter_optimizer.optimized_boosts,
            'kelly_fraction': kelly_fraction
        }

    def train_ml_model(self) -> bool:
        """Train or retrain ML model."""
        if not self.ml_enhancer:
            return False

        return self.ml_enhancer.train_model(self.ml_enhancer.training_data)

    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status."""
        status = {
            'total_decisions': self.total_decisions,
            'profitable_decisions': self.profitable_decisions,
            'win_rate': self.profitable_decisions / self.total_decisions if self.total_decisions > 0 else 0,
            'ml_enabled': self.enable_ml,
            'ml_trained': self.ml_enhancer.is_trained if self.ml_enhancer else False,
            'monitoring_enabled': self.enable_monitoring
        }

        if self.performance_monitor:
            status['performance_summary'] = self.performance_monitor.get_performance_summary()

        status['parameter_optimization'] = self.parameter_optimizer.get_optimized_parameters()
        status['correlation_regime'] = self.correlation_detector.get_regime_statistics()
        status['risk_budget'] = self.risk_budget.get_budget_summary()

        return status


# =============================================================================
# FACTORY FUNCTION
# =============================================================================

def create_production_phase5_system(
    base_system: Any = None,
    enable_ml: bool = True,
    enable_monitoring: bool = True
) -> ProductionPhase5System:
    """
    Factory function to create production Phase 5 system.

    Args:
        base_system: Optional base Phase 5 system
        enable_ml: Enable ML enhancement
        enable_monitoring: Enable performance monitoring

    Returns:
        Configured ProductionPhase5System
    """
    return ProductionPhase5System(
        base_system=base_system,
        enable_ml=enable_ml,
        enable_monitoring=enable_monitoring
    )
